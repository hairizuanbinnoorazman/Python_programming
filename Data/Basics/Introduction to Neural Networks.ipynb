{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "This notebook contains some code to introduce the concept and intuition behind Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Perceptron\n",
    "\n",
    "In a neural network, you would have the linear portions which is calculated by a product of the input and weights matrix. A bias is added on that. In order to introduce non-linearity into the function, we could use something like a sigmoid here. \n",
    "\n",
    "Reason for the non-linear portion: Most problems in the world do not follow a linear relationship but most have a non-linear relationship with the independent variables. If the linear relationship only is desired, we could just have a huge X matrix multiplied by weights and a bias added to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "# This is a simple node in a neural network but it can't do any learning on its own yet. There is no component\n",
    "# available that would allow it to adjust the weights and bias and move it accordingly.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to introduce non linearity\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "output = sigmoid(np.dot(inputs, weights) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network with Gradient Descent\n",
    "\n",
    "Adding a Gradient descent step.\n",
    "This example assumes bias as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "# Utilize the chain rule (A bit not obvious)\n",
    "# Simplified form in mathematics: np.exp(-x)/(sigmoid(x)^2)\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.array([0.1, 0.3])\n",
    "y = 0.2\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "# Hyperparameter\n",
    "learnrate = 0.5\n",
    "\n",
    "# The neural network output\n",
    "nn_output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "# output error\n",
    "error = y - nn_output\n",
    "\n",
    "# error gradient\n",
    "error_grad = error * sigmoid_prime(np.dot(x,weights))\n",
    "\n",
    "# Gradient descent step\n",
    "del_w = learnrate * error_grad * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network with Gradient Descent in Loop\n",
    "\n",
    "This is the one neural network that kind of works. So with loops, you will essentially keep looping the section to reduce the mean sum squared error for the network. However, the following example is still a single node neural network and it's uses is quite limited.\n",
    "\n",
    "Accuracy wise, this section won't be able to perform too many micacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(weights, x))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate change in weights\n",
    "        del_w += error * output * (1 - output) * x\n",
    "\n",
    "        # TODO: Update weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
